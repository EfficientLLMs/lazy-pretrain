{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g3DchNO9q-xT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vmasti/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, default_data_collator, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import GradScalerKwargs\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PbQzwX2TrUQx"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\n",
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params, trainable_params\n",
        "\n",
        "\n",
        "def seed_all(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgM4ohy5OAYK"
      },
      "source": [
        "## Get the training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-vac6TSODSi",
        "outputId": "5e5cc5c9-c23d-43f6-cae1-e731542bdc33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'lazy-pretrain'...\n",
            "remote: Enumerating objects: 1035, done.\u001b[K\n",
            "remote: Counting objects: 100% (493/493), done.\u001b[K\n",
            "remote: Compressing objects: 100% (193/193), done.\u001b[K\n",
            "remote: Total 1035 (delta 405), reused 381 (delta 299), pack-reused 542 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1035/1035), 35.89 MiB | 18.02 MiB/s, done.\n",
            "Resolving deltas: 100% (785/785), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/EfficientLLMs/lazy-pretrain.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCcUzONgSiAL",
        "outputId": "a9f438dc-79e0-4e61-99fd-04b5bb53c4e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/17)\u001b[K\rremote: Counting objects:  11% (2/17)\u001b[K\rremote: Counting objects:  17% (3/17)\u001b[K\rremote: Counting objects:  23% (4/17)\u001b[K\rremote: Counting objects:  29% (5/17)\u001b[K\rremote: Counting objects:  35% (6/17)\u001b[K\rremote: Counting objects:  41% (7/17)\u001b[K\rremote: Counting objects:  47% (8/17)\u001b[K\rremote: Counting objects:  52% (9/17)\u001b[K\rremote: Counting objects:  58% (10/17)\u001b[K\rremote: Counting objects:  64% (11/17)\u001b[K\rremote: Counting objects:  70% (12/17)\u001b[K\rremote: Counting objects:  76% (13/17)\u001b[K\rremote: Counting objects:  82% (14/17)\u001b[K\rremote: Counting objects:  88% (15/17)\u001b[K\rremote: Counting objects:  94% (16/17)\u001b[K\rremote: Counting objects: 100% (17/17)\u001b[K\rremote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 9 (delta 7), reused 9 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  11% (1/9)\rUnpacking objects:  22% (2/9)\rUnpacking objects:  33% (3/9)\rUnpacking objects:  44% (4/9)\rUnpacking objects:  55% (5/9)\rUnpacking objects:  66% (6/9)\rUnpacking objects:  77% (7/9)\rUnpacking objects:  88% (8/9)\rUnpacking objects: 100% (9/9)\rUnpacking objects: 100% (9/9), 2.52 KiB | 859.00 KiB/s, done.\n",
            "From https://github.com/EfficientLLMs/lazy-pretrain\n",
            "   1e5fda4..1e3546d  main       -> origin/main\n",
            "Updating 1e5fda4..1e3546d\n",
            "Fast-forward\n",
            " run_scripts/pretrain_lora.sh  |  43 \u001b[32m++++++++++\u001b[m\u001b[31m----\u001b[m\n",
            " run_scripts/run_grow.sh       |  28 \u001b[32m+++++\u001b[m\u001b[31m-----\u001b[m\n",
            " src/pretrain/pretrain_lora.py | 126 \u001b[32m+++++++++++++++++++++++++++++++\u001b[m\u001b[31m-----------\u001b[m\n",
            " src/pretrain/utils.py         |  25 \u001b[32m+++++++\u001b[m\u001b[31m--\u001b[m\n",
            " 4 files changed, 157 insertions(+), 65 deletions(-)\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EObfb9L1OGdB",
        "outputId": "aa21e3e3-e3da-4e22-9792-1089a5c0ded1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/vmasti/efficient-llms-capstone/lazy-pretrain\n"
          ]
        }
      ],
      "source": [
        "%cd lazy-pretrain/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXOiU0PLMfDE"
      },
      "source": [
        "## Download and use the pre-tokenized data with the same order\n",
        "EleutherAI has provided a pre-tokenized version of the standard (duplicated) pile dataset, which is also Pythia pre-shuffled. The dataset contains only token_ids. [link](https://huggingface.co/datasets/EleutherAI/pile-standard-pythia-preshuffled/tree/main)\n",
        "\n",
        "The whole dataset has about 300B tokens. `00.bin` to `19.bin` are about 30GB large each. The last one `20.bin` is only 78.3MB. We can download only the last one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74K6FpLeMoi3"
      },
      "source": [
        "1. Clone the repository without downloading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkpmhQdgMrCK",
        "outputId": "c5d0f57e-bcc7-4169-84aa-5ae9aad632ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pile-standard-pythia-preshuffled'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 30 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (30/30), 4.99 KiB | 852.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!mkdir data && cd data && GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/EleutherAI/pile-standard-pythia-preshuffled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfOKXBpuMvSh"
      },
      "source": [
        "2. Download only the last file, which has 39_168_000 tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boaKGvTzMvFc",
        "outputId": "a90af541-1726-466f-bee4-a7116f0122f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "!cd data/pile-standard-pythia-preshuffled && git lfs pull --include=\"document-00020-of-00020.bin\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51q4naj8M46N"
      },
      "source": [
        "3. Get all the tokens from the last file. We can use it to get the last 5M tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAb7jQQeMutE",
        "outputId": "b9e88774-ccfe-433f-88b5-e949c5e8a969"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "39168000"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filename = \"data/pile-standard-pythia-preshuffled/document-00020-of-00020.bin\"\n",
        "tokens = np.memmap(filename, dtype=np.uint16)\n",
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW9fEq28PaVb",
        "outputId": "db410277-2524-4546-e589-df5ec9474ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ai2-olmo in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (2.6.0+cu124)\n",
            "Requirement already satisfied: ai2-olmo-core==0.1.0 in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (0.1.0)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (2.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (13.9.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (1.38.14)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (2.19.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (0.21.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (24.2)\n",
            "Requirement already satisfied: cached_path>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (1.7.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (4.51.3)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from ai2-olmo) (6.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from ai2-olmo-core==0.1.0->ai2-olmo) (0.5.3)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from ai2-olmo-core==0.1.0->ai2-olmo) (2.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ai2-olmo-core==0.1.0->ai2-olmo) (2.32.3)\n",
            "Requirement already satisfied: filelock<4.0,>=3.4 in /usr/local/lib/python3.11/dist-packages (from cached_path>=1.6.2->ai2-olmo) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from cached_path>=1.6.2->ai2-olmo) (0.31.1)\n",
            "Requirement already satisfied: botocore<1.39.0,>=1.38.14 in /usr/local/lib/python3.11/dist-packages (from boto3->ai2-olmo) (1.38.14)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->ai2-olmo) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from boto3->ai2-olmo) (0.12.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->ai2-olmo) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->ai2-olmo) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->ai2-olmo) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->ai2-olmo) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->ai2-olmo) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->ai2-olmo) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->ai2-olmo) (2.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->ai2-olmo) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->ai2-olmo) (1.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->ai2-olmo) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf->ai2-olmo) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->ai2-olmo) (2024.11.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->ai2-olmo) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.14->boto3->ai2-olmo) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.14->boto3->ai2-olmo) (2.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->ai2-olmo) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->ai2-olmo) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->ai2-olmo) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (4.9.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.8.1->cached_path>=1.6.2->ai2-olmo) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->ai2-olmo) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.0->ai2-olmo-core==0.1.0->ai2-olmo) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.0->ai2-olmo-core==0.1.0->ai2-olmo) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.0->ai2-olmo-core==0.1.0->ai2-olmo) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ai2-olmo-core==0.1.0->ai2-olmo) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ai2-olmo-core==0.1.0->ai2-olmo) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ai2-olmo-core==0.1.0->ai2-olmo) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->ai2-olmo) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.14->boto3->ai2-olmo) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ai2-olmo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afjMcXm1OPei"
      },
      "source": [
        "## Grow the 400m model to 1.4b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XNqJvtoOR4x",
        "outputId": "9e6d81c5-f292-4804-edb0-4e9fe3cd627d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model: 24 layers, 1024 width\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "/home/vmasti/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Finish the following sentence:\n",
            "Raindrops on roses,\n",
            "\n",
            "A:\n",
            "\n",
            "I think\n",
            "\n",
            "Expanding model of 1024 width to 2048 width\n",
            "attention ratio: 1.0\n",
            "Grown model: 24 layers, 2048 width\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Finish the following sentence:\n",
            "Raindrops on roses,\n",
            "\n",
            "A:\n",
            "\n",
            "I think\n",
            "\n",
            "Grown model config: GPTNeoXConfig {\n",
            "  \"_name_or_path\": \"EleutherAI/pythia-410m-expand-width-2048\",\n",
            "  \"architectures\": [\n",
            "    \"GPTNeoXForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": true,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"gpt_neox\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"partial_rotary_factor\": 0.25,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 0.25,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_parallel_residual\": true,\n",
            "  \"vocab_size\": 50304\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python src/grow/grow.py \\\n",
        "    --small_model \"pythia-410m\" \\\n",
        "    --large_depth 24 \\\n",
        "    --large_width 2048 \\\n",
        "    --depth_growth \"alternate\" \\\n",
        "    --attn_heads 16 \\\n",
        "    --output_dir \"models/pythia-410m-to-pythia-1.4b\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTqejHZdsBf0"
      },
      "source": [
        "# Pre-training a 1.4B model with limited compute using a 410m model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPJRREYSGABn",
        "outputId": "e43ab491-0128-4bb3-d242-aaf8910e1754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n",
            "\n",
            "Training configuration:\n",
            "Total batches: 306\n",
            "Number of GPUs: 1\n",
            "Steps per GPU: 306\n",
            "Total steps: 306\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvibhamasti\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.19.11 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/vmasti/efficient-llms-capstone/lazy-pretrain/wandb/run-20250515_011333-ixiw0xmc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpythia-410-1.4b-10m-tokens\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vibhamasti/lora-pretraining\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vibhamasti/lora-pretraining/runs/ixiw0xmc/workspace\u001b[0m\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.61s/it]\n",
            "Memory after enabling gradient checkpointing: 2794.25 MB\n",
            "Model memory usage: 2794.25 MB\n",
            "Max memory allocated: 2794.25 MB\n",
            "Memory reserved: 2810.00 MB\n",
            "Total trainable parameters: 50331648\n",
            "Train ratio: 0.0344\n",
            "base_model.model.gpt_neox.embed_in.weight: requires_grad = False, shape = torch.Size([50304, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.0.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.1.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.1.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.2.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.2.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.3.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.3.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.4.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.4.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.5.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.5.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.6.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.6.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.7.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.7.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.8.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.8.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.9.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.9.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.10.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.10.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.11.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.11.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.12.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.12.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.13.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.13.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.14.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.14.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.15.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.15.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.16.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.16.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.16.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.17.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.17.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.17.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.18.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.18.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.18.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.19.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.19.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.19.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.20.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.20.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.20.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.21.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.21.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.21.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.22.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.22.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.22.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.input_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.input_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.post_attention_layernorm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.post_attention_layernorm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.attention.query_key_value.base_layer.weight: requires_grad = False, shape = torch.Size([6144, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.attention.query_key_value.base_layer.bias: requires_grad = False, shape = torch.Size([6144]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.attention.query_key_value.lora_A.default.weight: requires_grad = True, shape = torch.Size([256, 2048]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.23.attention.query_key_value.lora_B.default.weight: requires_grad = True, shape = torch.Size([6144, 256]), type = torch.float32\n",
            "base_model.model.gpt_neox.layers.23.attention.dense.weight: requires_grad = False, shape = torch.Size([2048, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.attention.dense.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.mlp.dense_h_to_4h.weight: requires_grad = False, shape = torch.Size([8192, 2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.mlp.dense_h_to_4h.bias: requires_grad = False, shape = torch.Size([8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.mlp.dense_4h_to_h.weight: requires_grad = False, shape = torch.Size([2048, 8192]), type = torch.float16\n",
            "base_model.model.gpt_neox.layers.23.mlp.dense_4h_to_h.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.final_layer_norm.weight: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.gpt_neox.final_layer_norm.bias: requires_grad = False, shape = torch.Size([2048]), type = torch.float16\n",
            "base_model.model.embed_out.weight: requires_grad = False, shape = torch.Size([50304, 2048]), type = torch.float16\n",
            "Start training...\n",
            "  0%|                                                   | 0/306 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "Training step 0, loss: 2.13, ppl: 8.39\n",
            " 33%|█████████████▍                           | 100/306 [09:16<19:09,  5.58s/it]Training step 100, loss: 2.27, ppl: 9.72\n",
            " 65%|██████████████████████████▊              | 200/306 [18:35<09:54,  5.61s/it]Training step 200, loss: 2.24, ppl: 9.40\n",
            " 98%|████████████████████████████████████████▏| 300/306 [27:55<00:33,  5.59s/it]Training step 300, loss: 2.23, ppl: 9.34\n",
            "100%|█████████████████████████████████████████| 306/306 [28:24<00:00,  5.57s/it]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B sync reduced upload amount by 6.3%             \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          loss ▆▂▅▂▇▂▇▃▂▅▆▃█▄▃▃▇▄▅▆▄▆▃▁▁▄▂▂▄▃▅▅▃▅▂▂▆▂▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           ppl ▆▂▅▁▇▂▇▃▂▅▅▂█▄▃▃▆▃▄▅▄▅▃▁▁▄▂▂▃▃▅▅▂▄▂▂▅▂▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          loss 3.88624\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           ppl 48.7272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          step 305\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        tokens 10000384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpythia-410-1.4b-10m-tokens\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/vibhamasti/lora-pretraining/runs/ixiw0xmc/workspace\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_011333-ixiw0xmc/logs\u001b[0m\n",
            "Training finished. Average loss: 2.30, Average PPL: 10.22\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "\n",
        "!python src/pretrain/pretrain_lora.py \\\n",
        "    --grown_model \"models/pythia-410m-to-pythia-1.4b\" \\\n",
        "    --tokenizer \"EleutherAI/pythia-70m\" \\\n",
        "    --seed 1234 \\\n",
        "    --rank 256 \\\n",
        "    --lora_alpha 256 \\\n",
        "    --batch_size 32 \\\n",
        "    --lr 1e-5 \\\n",
        "    --output_dir \"models/pythia-410m-to-pythia-1.4b-lora-10m\" \\\n",
        "    --dataset 'pile' \\\n",
        "    --num_tokens 10_000_000 \\\n",
        "    --chunk_size 1024 \\\n",
        "    --use_on_the_fly \\\n",
        "    --first_idx 19 \\\n",
        "    --last_idx 20 \\\n",
        "    --wandb_entity vibhamasti \\\n",
        "    --wandb_run_name \"pythia-410-1.4b-10m-tokens\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qCX9NIbPJOj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
